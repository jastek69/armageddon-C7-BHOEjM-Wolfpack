# Class 7 Armageddon - Brotherhood of Evil jerMutants - Wolfpack

![AWS](https://img.shields.io/badge/Cloud-AWS-orange?logo=amazon-aws&logoColor=white)
![EC2](https://img.shields.io/badge/Service-EC2-blue?logo=amazon-aws&logoColor=white)
![Git](https://img.shields.io/badge/Git-Branching-yellow?logo=git&logoColor=white)

![Status 1a](https://img.shields.io/badge/Lab1a-success?logo=github)
![Status 1b](https://img.shields.io/badge/Lab1b-success?logo=github)
![Status 1cA](https://img.shields.io/badge/Lab1c_bonus-A-success?logo=github)
![Status 1cB](https://img.shields.io/badge/Lab1c_bonus-B-success?logo=github)
![Status 1cC](https://img.shields.io/badge/Lab1c_bonus-C-success?logo=github)
![Status 1cD](https://img.shields.io/badge/Lab1c_bonus-D-success?logo=github)
![Status 1cE](https://img.shields.io/badge/Lab1c_bonus-E-success?logo=github)
![Status 1cF](https://img.shields.io/badge/Lab1c_bonus-F-success?logo=github)
![Status 1cG](https://img.shields.io/badge/Lab1c_bonus-G-success?logo=github)
![Status 1cH](https://img.shields.io/badge/Lab1c_bonus-H-success?logo=github)
![Status 1cI](https://img.shields.io/badge/Lab1c_bonus-I-success?logo=github)
![Status 2a](https://img.shields.io/badge/Lab2a-success?logo=github)
![Status 2b](https://img.shields.io/badge/Lab2b-success?logo=github)
![Status 3a](https://img.shields.io/badge/Lab3a-success?logo=github)
![Status 3b](https://img.shields.io/badge/Lab3b-success?logo=github)
![Status 4](https://img.shields.io/badge/Lab4-inprogress?logo=github)



This repository contains links to repos of group members for multiple **Labs for Class 7 Armageddon (1a, 1b, 1c, 1d, 1e, 1f, 1g, 1h, 1I, 2a, 2b, 3a, 3b, and 4)**.
Each LAB is tracked in its own branch with unique deliverables and tasks.

---

# [üåê LAB1]
## LAB1A-B

#### Deliverables
* [T.I.Q.S]
* [John Sweeney - Deliverable](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations/tree/LAB2/LAB1-DELIVERABLES))
* [Ernest Morris - Deliverables](https://github.com/jastek69/armageddon-C7-BHOEjM-Wolfpack.git)
* 



### [Lab1a Explanation](https://github.com/BalericaAI/armageddon/blob/main/SEIR_Foundations/LAB1/1a_explanation.md?plain=1)
Project Overview (What You Are Building)
In this lab, you will build a classic cloud application architecture:
A compute layer running on an Amazon EC2 instance
A managed relational database hosted on Amazon RDS
Secure connectivity between the two using VPC networking and security groups
Credential management using AWS Secrets Manager
A simple application that writes and reads data from the database

### [Lab1b - explanation](https://github.com/BalericaAI/armageddon/blob/main/SEIR_Foundations/LAB1/1b_Incident_response.md?plain=1)
Lab 1b ‚Äî Incident Response Scenario
Prerequisite: Lab 1a + Lab 1b infrastructure completed

PART I ‚Äî Incident Scenario (What Messed Up)
Incident Title
Database Connectivity Failure ‚Äî Production Application Unavailable
Symptoms Reported
    Application intermittently returns errors
    /list endpoint fails or hangs
    No recent code changes
    EC2 instance is still running

Constraints
  You may NOT:
      Recreate EC2
      Recreate RDS
      Hardcode credentials

You MUST:
    Use logs
    Use alarms
    Use stored configuration values

PART II ‚Äî Incident Injection (Group Leader / Auto-Grader)
Choose one of the following to trigger the incident:
  Option A ‚Äî Secret Drift (Recommended)
    Change DB password in Secrets Manager
    Do not update the actual RDS password

  Option B ‚Äî Network Isolation
    Remove EC2 security group from RDS inbound rule (TCP 3306)

  Option C ‚Äî DB Interruption
    Stop RDS instance temporarily

Students are not told which failure was injected.

PART III ‚Äî Monitoring & Alerting (SNS + PagerDuty Simulation)
  1. SNS Alert Channel
    SNS Topic
    Name: lab-db-incidents
    aws sns create-topic --name lab-db-incidents
    Email Subscription (PagerDuty Simulation)

          aws sns subscribe \
            --topic-arn <TOPIC_ARN> \
            --protocol email \
            --notification-endpoint your-email@example.com

    This simulates PagerDuty / OpsGenie paging an engineer.

2. CloudWatch Alarm ‚Üí SNS
Alarm Concept
Trigger when:
  DB connection errors ‚â• 3 in 5 minutes
Alarm Creation (example)

        aws cloudwatch put-metric-alarm \
          --alarm-name lab-db-connection-failure \
          --metric-name DBConnectionErrors \
          --namespace Lab/RDSApp \
          --statistic Sum \
          --period 300 \
          --threshold 3 \
          --comparison-operator GreaterThanOrEqualToThreshold \
          --evaluation-periods 1 \
          --alarm-actions <SNS_TOPIC_ARN>

Expected Behavior
  Alarm transitions to ALARM
  SNS notification sent
  Student receives alert email

PART IV ‚Äî Mandatory Incident Runbook
Students must follow this order. Deviations lose points.

RUNBOOK SECTION 1 ‚Äî Acknowledge
1.1 Confirm Alert
  aws cloudwatch describe-alarms \
  --alarm-name lab-db-connection-failure \
  --query "MetricAlarms[].StateValue"

Expected:
  ALARM


RUNBOOK SECTION 2 ‚Äî Observe
2.1 Check Application Logs

      aws logs filter-log-events \
      --log-group-name /aws/ec2/lab-rds-app \
      --filter-pattern "ERROR"

Expected:
  Clear DB connection failure messages

2.2 Identify Failure Type
Students must classify:
  Credential failure?
  Network failure?
  Database availability failure?
This classification is graded.

RUNBOOK SECTION 3 ‚Äî Validate Configuration Sources
3.1 Retrieve Parameter Store Values
    
      aws ssm get-parameters \
        --names /lab/db/endpoint /lab/db/port /lab/db/name \
        --with-decryption

Expected:
  Endpoint + port returned

3.2 Retrieve Secrets Manager Values

      aws secretsmanager get-secret-value \
      --secret-id lab/rds/mysql

Expected:
  Username/password visible
  Compare against known-good state

RUNBOOK SECTION 4 ‚Äî Containment
4.1 Prevent Further Damage
  Do not restart EC2 blindly
  Do not rotate secrets again
  Do not redeploy infrastructure

Students must explicitly state:
  ‚ÄúSystem state preserved for recovery.‚Äù


RUNBOOK SECTION 5 ‚Äî Recovery
Recovery Paths (Depends on Root Cause)
    If Credential Drift
        Update RDS password to match Secrets Manager
        OR
        Update Secrets Manager to known-good value

    If Network Block
        Restore EC2 security group access to RDS on 3306

    If DB Stopped
        Start RDS and wait for available

Verify Recovery
    curl http://<EC2_PUBLIC_IP>/list

Expected:
    Application returns data
    No errors

RUNBOOK SECTION 6 ‚Äî Post-Incident Validation
6.1 Confirm Alarm Clears

    aws cloudwatch describe-alarms \
      --alarm-name lab-db-connection-failure \
      --query "MetricAlarms[].StateValue"

Expected:
    OK

6.2 Confirm Logs Normalize

    aws logs filter-log-events \
      --log-group-name /aws/ec2/lab-rds-app \
      --filter-pattern "ERROR"

Expected:
    No new errors


PART V ‚Äî Grading Rubric (100 Points)
| Category                       | Points |
| ------------------------------ | ------ |
| Alarm acknowledged via CLI     | 10     |
| Correct failure classification | 20     |
| Logs used correctly            | 15     |
| Parameter Store validated      | 10     |
| Secrets Manager validated      | 10     |
| Correct recovery action        | 20     |
| No redeploy / no guesswork     | 10     |
| Clear incident summary         | 5      |

PART VI ‚Äî Required Incident Report (Short)
Students must submit:
    Incident Summary
    What failed?
    How was it detected?
    Root cause
    Time to recovery

Preventive Action
    One improvement to reduce MTTR
    One improvement to prevent recurrence

PART VII ‚Äî What This Actually Teaches
By completing this lab, students demonstrate:
    On-call discipline
    Root cause analysis
    Cloud-native recovery
    Proper secret handling
    Operational maturity

This is exactly what separates:
    ‚ÄúI passed an AWS cert‚Äù
    from
    ‚ÄúI can be trusted with production.‚Äù


## Lab 1C ‚Äî Terraform: EC2 ‚Üí RDS + Secrets/Params + Observability + Incident Alerts
### LAB1C-H
#### Deliverables
* [John Sweeney - Deliverables 1C-I](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)


### Purpose
Modern companies do not build AWS by clicking around in the console.
They use Infrastructure as Code (IaC) so environments are repeatable, reviewable, auditable, and recoverable.

This repo is intentionally incomplete:
- It declares required resources
- Students must configure the details (rules, policies, user_data, app logging, etc.)

### Requirements (must exist in Terraform)
- VPC, public/private subnets, IGW, NAT, routing
- EC2 app host + IAM role/profile
- RDS (private) + subnet group + SG with inbound from EC2 SG
- Parameter Store values (/lab/db/*)
- Secrets Manager secret (db creds)
- CloudWatch log group
- CloudWatch alarm (DBConnectionErrors >= 3 per 5 min)
- SNS topic + subscription

### Student Deliverables
- `terraform plan` output
- `terraform apply` evidence (outputs)
- CLI verification commands (from Lab 1b)
- Incident runbook execution notes (alarm fired + recovered)

## LAB1: 1c_bonus-C
Below is the Route53 add-on for Bonus-B (Hosted Zone + ACM DNS validation records + app.chewbacca-growl.com ALIAS ‚Üí ALB).
It‚Äôs written as a Terraform skeleton with Chewbacca comment style.

Add this as bonus_b_route53.tf.

Add to variables.tf (append)
variable "manage_route53_in_terraform" {
  description = "If true, create/manage Route53 hosted zone + records in Terraform."
  type        = bool
  default     = true
}

variable "route53_hosted_zone_id" {
  description = "If manage_route53_in_terraform=false, provide existing Hosted Zone ID for domain."
  type        = string
  default     = ""
}

Add file: bonus_b_route53.tf

Important note about the HTTPS listener
In your earlier bonus_b.tf, your HTTPS listener referenced:

certificate_arn = aws_acm_certificate_validation.chewbacca_acm_validation01.certificate_arn

Now you have two possible validation resources (email/manual vs DNS). For the skeleton, do this pattern:
    Keep your original aws_acm_certificate_validation (email/manual) if you want
    OR switch the listener certificate ARN to the certificate itself and rely on validation dependency
Best skeleton approach (simple + works):
Update HTTPS listener to use:
  certificate_arn = aws_acm_certificate.chewbacca_acm_cert01.arn

‚Ä¶and keep depends_on pointing at the DNS validation resource when DNS mode is used.
I‚Äôll give you a clean patch below.

### Explanation: HTTPS listener is the real hangar bay ‚Äî TLS terminates here, then traffic goes to private targets.
resource "aws_lb_listener" "chewbacca_https_listener01" {
  load_balancer_arn = aws_lb.chewbacca_alb01.arn
  port              = 443
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-TLS13-1-2-2021-06"
  certificate_arn   = aws_acm_certificate.chewbacca_acm_cert01.arn

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.chewbacca_tg01.arn
  }

### TODO: If DNS validation is enabled, ensure validation completes before listener creation.
  depends_on = [
    aws_acm_certificate_validation.chewbacca_acm_validation01_dns_bonus
  ]
}


If you choose EMAIL validation instead, 
you can comment out the depends_on or set certificate_validation_method="EMAIL" 
and keep the listener creation after manual validation. (This is a skeleton; you‚Äôll get some ‚Äúlearning friction‚Äù here.)

Continue bonus_b_route53.tf ‚Äî ALIAS record app ‚Üí ALB

############################################
### ALIAS record: app.chewbacca-growl.com -> ALB
############################################

### Explanation: This is the holographic sign outside the cantina‚Äîapp.chewbacca-growl.com points to your ALB.
resource "aws_route53_record" "chewbacca_app_alias01" {
  zone_id = local.chewbacca_zone_id
  name    = local.chewbacca_app_fqdn
  type    = "A"

  alias {
    name                   = aws_lb.chewbacca_alb01.dns_name
    zone_id                = aws_lb.chewbacca_alb01.zone_id
    evaluate_target_health = true
  }
}

Add outputs (append to outputs.tf)
### Explanation: Outputs are the nav computer readout‚ÄîChewbacca needs coordinates that humans can paste into browsers.
output "chewbacca_route53_zone_id" {
  value = local.chewbacca_zone_id
}

output "chewbacca_app_url_https" {
  value = "https://${var.app_subdomain}.${var.domain_name}"
}

Student verification (CLI)
1) Confirm hosted zone exists (if managed)
  aws route53 list-hosted-zones-by-name \
    --dns-name chewbacca-growl.com \
    --query "HostedZones[].Id"

2) Confirm app record exists
  aws route53 list-resource-record-sets \
  --hosted-zone-id <ZONE_ID> \
  --query "ResourceRecordSets[?Name=='app.chewbacca-growl.com.']"

3) Confirm certificate issued
  aws acm describe-certificate \
  --certificate-arn <CERT_ARN> \
  --query "Certificate.Status"

Expected: ISSUED

4) Confirm HTTPS works
  curl -I https://app.chewbacca-growl.com

Expected: HTTP/1.1 200 (or 301 then 200 depending on your app)

What YOU must understand (career point)
This is exactly how companies do it:
  DNS points to ingress
  TLS via ACM
  ALB handles secure public entry
  private compute does the work
  WAF + alarms defend and alert

When students can Terraform this, they‚Äôre doing real cloud engineering.


## LAB1C-Bonus-D - Route53 Apex and Logging
#### Deliverables
* [John Sweeney - Deliverables](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)


Ready to Suffer? ‚Äîhere‚Äôs the next realism bump for Lab 1C-Bonus-D:
  1) Zone apex (chewbacca-growl.com) ALIAS ‚Üí ALB
  2) ALB access logs ‚Üí S3 bucket (with the required bucket policy)
  3) A couple of verification commands students can run to prove it‚Äôs working

Add this as bonus_b_logging_route53_apex.tf (or append to your existing Route53/logging file).

Add variables (append to variables.tf)
variable "enable_alb_access_logs" {
  description = "Enable ALB access logging to S3."
  type        = bool
  default     = true
}

variable "alb_access_logs_prefix" {
  description = "S3 prefix for ALB access logs."
  type        = string
  default     = "alb-access-logs"
}

Add file: bonus_b_logging_route53_apex.tf (go to Folder)

Patch reminder (students must modify the existing ALB resource)
Terraform can‚Äôt ‚Äúappend‚Äù nested blocks, so they must edit:
In bonus_b.tf, inside resource "aws_lb" "chewbacca_alb01" { ... } add:

  # Explanation: Chewbacca keeps flight logs‚ÄîALB access logs go to S3 for audits and incident response.
  access_logs {
    bucket  = aws_s3_bucket.chewbacca_alb_logs_bucket01[0].bucket
    prefix  = var.alb_access_logs_prefix
    enabled = var.enable_alb_access_logs
  }

Outputs (append to outputs.tf)

# Explanation: The apex URL is the front gate‚Äîhumans type this when they forget subdomains.
output "chewbacca_apex_url_https" {
  value = "https://${var.domain_name}"
}

# Explanation: Log bucket name is where the footprints live‚Äîuseful when hunting 5xx or WAF blocks.
output "chewbacca_alb_logs_bucket_name" {
  value = var.enable_alb_access_logs ? aws_s3_bucket.chewbacca_alb_logs_bucket01[0].bucket : null
}

Student verification (CLI) ‚Äî DNS + Logs
1) Verify apex record exists
  aws route53 list-resource-record-sets \
    --hosted-zone-id <ZONE_ID> \
    --query "ResourceRecordSets[?Name=='chewbacca-growl.com.']"

2) Verify ALB logging is enabled
  aws elbv2 describe-load-balancers \
    --names chewbacca-alb01 \
    --query "LoadBalancers[0].LoadBalancerArn"

Then:
  aws elbv2 describe-load-balancer-attributes \
  --load-balancer-arn <ALB_ARN>

  Expected attributes include:
  access_logs.s3.enabled = true
  access_logs.s3.bucket = your bucket
  access_logs.s3.prefix = your prefix

3) Generate some traffic
  curl -I https://chewbacca-growl.com
  curl -I https://app.chewbacca-growl.com

4) Verify logs arrived in S3 (may take a few minutes)
  aws s3 ls s3://<BUCKET_NAME>/<PREFIX>/AWSLogs/<ACCOUNT_ID>/elasticloadbalancing/ --recursive | head


Why this matters to YOU (career-critical point)
This is incident response fuel:
  Access logs tell you:
    client IPs
    paths
    response codes
    target behavior
    latency

Combined with WAF logs/metrics and ALB 5xx alarms, you can do real triage:
  ‚ÄúIs it attackers, misroutes, or downstream failure?‚Äù




## LAB1C-Bonus-E - WAF
#### Deliverables
* [John Sweeney - Deliverables](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)

Key update since ‚Äúthe old days‚Äù: AWS WAF logging can go directly to CloudWatch Logs, S3, or Kinesis Data Firehose, 
and you can associate one destination per Web ACL. Also, the destination name must start with aws-waf-logs-. 


Terraform supports this with aws_wafv2_web_acl_logging_configuration. 
Terraform Registry

Below is Lab 1C-Bonus-E (continued): WAF logging in Terraform (with toggles), plus verification commands.

1) Add variables (append to variables.tf)
variable "waf_log_destination" {
  description = "Choose ONE destination per WebACL: cloudwatch | s3 | firehose"
  type        = string
  default     = "cloudwatch"
}

variable "waf_log_retention_days" {
  description = "Retention for WAF CloudWatch log group."
  type        = number
  default     = 14
}

variable "enable_waf_sampled_requests_only" {
  description = "If true, students can optionally filter/redact fields later. (Placeholder toggle.)"
  type        = bool
  default     = false
}


2) Add file: bonus_b_waf_logging.tf (Look in Folder)

This provides three skeleton options (CloudWatch / S3 / Firehose). Students choose one via var.waf_log_destination.


3) Outputs (append to outputs.tf)
# Explanation: Coordinates for the WAF log destination‚ÄîChewbacca wants to know where the footprints landed.
output "chewbacca_waf_log_destination" {
  value = var.waf_log_destination
}

output "chewbacca_waf_cw_log_group_name" {
  value = var.waf_log_destination == "cloudwatch" ? aws_cloudwatch_log_group.chewbacca_waf_log_group01[0].name : null
}

output "chewbacca_waf_logs_s3_bucket" {
  value = var.waf_log_destination == "s3" ? aws_s3_bucket.chewbacca_waf_logs_bucket01[0].bucket : null
}

output "chewbacca_waf_firehose_name" {
  value = var.waf_log_destination == "firehose" ? aws_kinesis_firehose_delivery_stream.chewbacca_waf_firehose01[0].name : null
}


4) Student verification (CLI)
A) Confirm WAF logging is enabled (authoritative)
  aws wafv2 get-logging-configuration \
    --resource-arn <WEB_ACL_ARN>

Expected: LogDestinationConfigs contains exactly one destination.

B) Generate traffic (hits + blocks)
  curl -I https://chewbacca-growl.com/
  curl -I https://app.chewbacca-growl.com/

C1) If CloudWatch Logs destination
  aws logs describe-log-streams \
  --log-group-name aws-waf-logs-<project>-webacl01 \
  --order-by LastEventTime --descending

Then pull recent events:
  aws logs filter-log-events \
  --log-group-name aws-waf-logs-<project>-webacl01 \
  --max-items 20

C2) If S3 destination
  aws s3 ls s3://aws-waf-logs-<project>-<account_id>/ --recursive | head

C3) If Firehose destination
  aws firehose describe-delivery-stream \
  --delivery-stream-name aws-waf-logs-<project>-firehose01 \
  --query "DeliveryStreamDescription.DeliveryStreamStatus"

And confirm objects land:
  aws s3 ls s3://<firehose_dest_bucket>/waf-logs/ --recursive | head

5) Why this makes incident response ‚Äúreal‚Äù
Now you can answer questions like:
  ‚ÄúAre 5xx caused by attackers or backend failure?‚Äù
  ‚ÄúDo we see WAF blocks spike before ALB 5xx?‚Äù
  ‚ÄúWhat paths / IPs are hammering the app?‚Äù
  ‚ÄúIs it one client, one ASN, one country, or broad?‚Äù
  ‚ÄúDid WAF mitigate, or are we failing downstream?‚Äù

This is precisely why WAF logging destinations include CloudWatch Logs (fast search) and S3/Firehose (archive/SIEM pipeline)










## LAB1C-Bonus-F Cloudwatch
#### Deliverables
* [John Sweeney - Deliverables](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)

Here‚Äôs a CloudWatch Logs Insights query pack you can drop straight into the Lab 1C-Bonus-B incident runbook.

Two important notes up front:
  CloudWatch Logs Insights works only on logs that are in CloudWatch Logs.
  So this pack covers:
    WAF logs (when you chose waf_log_destination="cloudwatch")
    App logs (your /aws/ec2/<prefix>-rds-app group)

  ALB access logs are in S3, not CloudWatch Logs (unless you also ship them to CW via another pipeline).
    For ALB, you‚Äôll correlate via:
      CloudWatch metrics (5xx alarm + metrics)
      and optionally Athena later (if you want the full CBRE-style ‚Äúlog lake‚Äù workflow)


Lab 1C-Bonus-F: Logs Insights Query Pack
Variables students fill in (for the runbook)
  WAF log group: aws-waf-logs-<project>-webacl01
  App log group: /aws/ec2/<project>-rds-app

Requirements: Set the time range to Last 15 minutes (or match incident window).

A) WAF Queries (CloudWatch Logs Insights)
A1) ‚ÄúWhat‚Äôs happening right now?‚Äù (Top actions: ALLOW/BLOCK)
  fields @timestamp, action
  | stats count() as hits by action
  | sort hits desc

A2) Top client IPs (who is hitting us the most?)
  fields @timestamp, httpRequest.clientIp as clientIp
| stats count() as hits by clientIp
| sort hits desc
| limit 25

A3) Top requested URIs (what are they trying to reach?)
  fields @timestamp, httpRequest.uri as uri
| stats count() as hits by uri
| sort hits desc
| limit 25

A4) Blocked requests only (who/what is being blocked?)
  fields @timestamp, action, httpRequest.clientIp as clientIp, httpRequest.uri as uri
| filter action = "BLOCK"
| stats count() as blocks by clientIp, uri
| sort blocks desc
| limit 25

A5) Which WAF rule is doing the blocking?
  fields @timestamp, action, terminatingRuleId, terminatingRuleType
| filter action = "BLOCK"
| stats count() as blocks by terminatingRuleId, terminatingRuleType
| sort blocks desc
| limit 25

A6) Rate of blocks over time (did it spike?)
  fields @timestamp, httpRequest.clientIp as clientIp, httpRequest.uri as uri
| filter uri like /wp-login|xmlrpc|\.env|admin|phpmyadmin|\.git|\/login/i
| stats count() as hits by clientIp, uri
| sort hits desc
| limit 50

#edit
fields @timestamp, httpRequest.clientIp as clientIp, httpRequest.uri as uri | filter uri =~ /wp-login|xmlrpc|\.env|admin|phpmyadmin|\.git|login/ | stats count() as hits by clientIp, uri | sort hits desc | limit 50

A7) Suspicious scanners (common patterns: admin paths, wp-login, etc.)
  fields @timestamp, httpRequest.clientIp as clientIp, httpRequest.uri as uri
| filter uri like /wp-login|xmlrpc|\.env|admin|phpmyadmin|\.git|\/login/i
| stats count() as hits by clientIp, uri
| sort hits desc
| limit 50

A8) Country/geo (if present in your WAF logs)
Some WAF log formats include httpRequest.country. If yours does:
  fields @timestamp, httpRequest.country as country
| stats count() as hits by country
| sort hits desc
| limit 25

B) App Queries (EC2 app log group)
These assume your app logs include meaningful strings like ERROR, DBConnectionErrors, timeout, etc
(You should enforce this.)

B1) Count errors over time (this should line up with the alarm window)
  fields @timestamp, @message
| filter @message like /ERROR|Exception|Traceback|DB|timeout|refused/i
| stats count() as errors by bin(1m)
| sort bin(1m) asc

B2) Show the most recent DB failures (triage view)
  fields @timestamp, @message
| filter @message like /DB|mysql|timeout|refused|Access denied|could not connect/i
| sort @timestamp desc
| limit 50

B3) ‚ÄúIs it creds or network?‚Äù classifier hints
  Credentials drift often shows: Access denied, authentication failures
  Network/SecurityGroup often shows: timeout, refused, ‚Äúno route‚Äù, hang
  fields @timestamp, @message
| filter @message like /Access denied|authentication failed|timeout|refused|no route|could not connect/i
| stats count() as hits by
  case(
    @message like /Access denied|authentication failed/i, "Creds/Auth",
    @message like /timeout|no route/i, "Network/Route",
    @message like /refused/i, "Port/SG/ServiceRefused",
    "Other"
  )
| sort hits desc


B4) Extract structured fields (Requires log JSON)
If you log JSON like: {"level":"ERROR","event":"db_connect_fail","reason":"timeout"}:
  fields @timestamp, level, event, reason
| filter level="ERROR"
| stats count() as n by event, reason
| sort n desc

(Thou Shalt need to emit JSON logs for this one.)

C) Correlation ‚ÄúEnterprise-style‚Äù mini-workflow (Runbook Section)
Add this to the incident runbook:

Step 1 ‚Äî Confirm signal timing
  CloudWatch alarm time window: last 5‚Äì15 minutes
  Run App B1 to see error spike time bins

Step 2 ‚Äî Decide: Attack vs Backend Failure
  Run WAF A1 + A6:
    If BLOCK spikes align with incident time ‚Üí likely external pressure/scanning
    If WAF is quiet but app errors spike ‚Üí likely backend (RDS/SG/creds)

Step 3 ‚Äî If backend failure suspected
  Run App B2 and classify:
    Access denied ‚Üí secrets drift / wrong password
    timeout ‚Üí SG/routing/RDS down
  Then retrieve known-good values:
    Parameter Store /lab/db/*
    Secrets Manager /<prefix>/rds/mysql

Step 4 ‚Äî Verify recovery
  App errors return to baseline (B1)
  WAF blocks stabilize (A6)
  Alarm returns to OK
  curl https://app.chewbacca-growl.com/list works














## LAB1C-Bonus-G Cloudwatch + Bedrock
#### Deliverables
* [John Sweeney - Deliverables](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)


Bedrock-powered ‚Äúauto-IR‚Äù pipeline for advanced students who can actually build and demo.

The pattern below is what real orgs do: alarm ‚Üí evidence collection ‚Üí LLM summarization ‚Üí report artifact ‚Üí notify.
This will be given to you

1) Incident report template (structured, consistent, gradeable)
2) Integration framework (AWS architecture + flow)
3) Terraform skeleton resources (Chewbacca naming)
4) Lambda handler skeleton (Python) that:
    pulls alarm context
    runs CloudWatch Logs Insights queries (WAF + app)
    pulls known-good values from SSM + Secrets
    calls Bedrock Runtime InvokeModel 
    writes report to S3
    notifies via SNS
Prompt pack (so reports are high signal, not vibes)

1) Auto-generated Incident Report Template (Markdown)
You must output exact headings (easy to implement).
--> 1c_bonus-G_Bedrock.template.md


2) Integration Framework (Bedrock ‚ÄúAuto-IR‚Äù)
Event-driven flow
    1) CloudWatch Alarm goes to ALARM
    2) Alarm triggers SNS topic (you already have this)
    3) SNS triggers a Lambda ‚ÄúIncidentReporter‚Äù
    4) Lambda:
        Pulls alarm metadata from event payload
        Runs Logs Insights queries via StartQuery/GetQueryResults 
        Fetches config from SSM and Secrets Manager
        Calls Bedrock Runtime to generate the report 
        Writes Markdown + JSON evidence bundle to S3
        Publishes a ‚ÄúReport Ready‚Äù message to SNS (link to S3 object)

Two modes (advanced students can implement both)
    Mode A: ‚ÄúFast report‚Äù (15-min window, small evidence)
    Mode B: ‚ÄúDeep report‚Äù (60-min window + WAF correlation + top URIs + error clustering)

Optional extra-credit: use Bedrock Agents + Knowledge Base to ingest your runbook and ‚Äúrecommend steps.‚Äù
# https://aws.amazon.com/blogs/machine-learning/automate-it-operations-with-amazon-bedrock-agents/?utm_source=chatgpt.com

3) Terraform Skeleton Add-on (Chewbacca naming)
Add file: bonus_G_bedrock_autoreport.tf (Folder)

4) Lambda ‚ÄúIncidentReporter‚Äù skeleton (Python)
Create handler.py and zip it for Terraform. (Folder)

This uses:
CloudWatch Logs Insights StartQuery/GetQueryResults 
Bedrock invoke_model via bedrock-runtime client

Note: Bedrock request body differs by model family; the framework is correct but students must adapt to the chosen model‚Äôs request schema. AWS documents InvokeModel and the runtime client.
Documentation: https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html?utm_source=chatgpt.com

5) Bedrock Prompt Pack (so reports don‚Äôt hallucinate)
Include these rules in the prompt (non-negotiable):
    ‚ÄúUse ONLY evidence‚Äù
    ‚ÄúIf unknown, say Unknown‚Äù
    ‚ÄúInclude confidence levels‚Äù
    ‚ÄúRecommend next evidence to pull‚Äù

And add a ‚Äúgrading‚Äù rubric line:
    ‚ÄúReport must cite which query/field supports each key claim‚Äù

6) Advanced grading criteria (for your top students)
They pass ‚Äúadvanced‚Äù if:
    They produce both JSON evidence + Markdown report
    Report has no invented claims
    Report includes root cause classification that matches injected failure
    They redact secrets (password never appears)
    They add a second pass: ‚Äúpreventive actions‚Äù tied to evidence (e.g., rotation automation, SG drift detection)

Optional upgrade: Bedrock Agents + Knowledge Base (very real)
You need to store:
    runbooks (Markdown)
    common incident patterns
        in a Knowledge Base, then an Agent can recommend steps. AWS has a reference blog for automating IT ops with Agents.
Documenation: https://aws.amazon.com/blogs/machine-learning/automate-it-operations-with-amazon-bedrock-agents/?utm_source=chatgpt.com








## LAB1C-Bonus-H Bedrock Auto-Generated Incident Reports
#### Deliverables
* [John Sweeney - Deliverables](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)


What you‚Äôre building:

When an alarm fires, you will automatically:
  1) collect evidence (alarm metadata + Logs Insights + param/secret reads)
  2) generate a structured incident report using Amazon Bedrock Runtime InvokeModel 
  3) store report + evidence bundle to S3
  4) notify the on-call engineer (SNS)

Why it matters
This is how mature companies reduce MTTR:
  A) evidence collection is automated (less guesswork)
  B) postmortems are consistent (better prevention)
  C) alerts include context (fewer ‚Äúwhat‚Äôs happening?‚Äù pages)

1) The ‚ÄúIntegration Contract‚Äù (what Lambda must output)
You must write two objects to S3:

A) Evidence bundle (JSON)
s3://<bucket>/reports/<incident_id>.json

Must contain:
  1) incident_id
  2) time_window_utc (start/end)
  3) alarm (name, metric, threshold, state)
  4) queries: results for WAF + app Logs Insights
  5) ssm_params (endpoint/port/name)
  6) secret_meta (host/port/dbname/username only ‚Äî no password)

B) Human report (Markdown)
s3://<bucket>/reports/<incident_id>.md

Must follow your template headings exactly.

2) The Logs Insights Query Pack (Minimum required)
Your Lambda must run at least these via Logs Insights:
  App: error rate over time (bin 1m)
  App: latest 50 DB-related error lines
  WAF: allow vs block
  WAF: top blocked IP/URI pairs

This is built on StartQuery + GetQueryResults.

3) Bedrock invocation: two supported paths (You need to pick one)

Critical reality: Bedrock request bodies differ per model provider/family. 
AWS explicitly warns that models differ in what they accept/return. 
Documentation: https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html?utm_source=chatgpt.com


Option 1: Anthropic Claude via Bedrock ‚Äúmessages‚Äù style payload
Use AWS‚Äôs own examples as the canonical reference. 
AWS Documentation: https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_InvokeModel_AnthropicClaude_section.html?utm_source=chatgpt.com


Python snippet (framework): claude.py (folder)

Option 2: ‚ÄúGeneric‚Äù InvokeModel (students adapt)
This exists to teach them to read provider-specific schemas and not cargo-cult. Start here, then adapt based on the model chosen

4) Lambda packaging: the clean ‚Äúclass-safe‚Äù way
Directory

lambda_ir_reporter/
  handler.py
  requirements.txt   (optional)
  build.sh
  lambda_ir_reporter.zip


build.sh (students run locally)


#!/usr/bin/env bash
set -euo pipefail

rm -rf build lambda_ir_reporter.zip
mkdir -p build

cp handler.py build/

# If you add external deps (usually you don't need any for boto3):
# pip install -r requirements.txt -t build/

cd build
zip -r ../lambda_ir_reporter.zip .
cd ..


Then in Terraform:
  filename = "lambda_ir_reporter.zip"

5) ‚ÄúFake alarm event‚Äù test harness (no waiting for real alarms)
SNS ‚Üí Lambda payloads can vary, so students should make Lambda accept:
  direct SNS event (Records[0].Sns.Message)
  or direct test JSON

Local test event (paste into Lambda console ‚ÄúTest‚Äù)

{
  "Records": [
    {
      "Sns": {
        "Subject": "ALARM: chewbacca-alb-5xx-alarm01",
        "Message": "{\"AlarmName\":\"chewbacca-alb-5xx-alarm01\",\"NewStateValue\":\"ALARM\",\"NewStateReason\":\"Threshold crossed\",\"StateChangeTime\":\"2025-12-27T16:00:00Z\"}"
      }
    }
  ]
}

Lambda parsing pattern (required) 

def parse_alarm_event(event):
    # SNS wrapped?
    if "Records" in event and event["Records"] and "Sns" in event["Records"][0]:
        msg = event["Records"][0]["Sns"]["Message"]
        try:
            return json.loads(msg)
        except json.JSONDecodeError:
            return {"raw_message": msg}
    return event

6) How students validate success (objective)
A) Confirm Lambda invoked
  aws logs tail /aws/lambda/<function-name> --since 10m

B) Confirm report objects exist
  aws s3 ls s3://<REPORT_BUCKET>/reports/ --recursive | tail

C) Open the report
  aws s3 cp s3://<REPORT_BUCKET>/reports/<incident_id>.md -

D) Confirm the evidence bundle does NOT include secrets
  aws s3 cp s3://<REPORT_BUCKET>/reports/<incident_id>.json - | grep -i password && echo "FAIL"


7) ‚ÄúNo hallucinations‚Äù enforcement (advanced requirement)
Inside the prompt you give Bedrock, include:
  ‚ÄúUse only evidence‚Äù
  ‚ÄúIf unknown, say Unknown‚Äù
  ‚ÄúCite the evidence key used for each claim‚Äù

This matches the Bedrock guidance that you pass model-specific inference parameters in the request body‚Äîstudents must shape the prompt accordingly.

8) Add the missing Terraform glue (SNS publish + report-ready)
You already have SNS and Lambda wired. Add a second SNS message (optional but fun):
  Subject: IR Report Ready
  Message: S3 path + incident_id

This is just sns.publish(...) (you already used it).

9) Upgrade path (extra credit, very ‚Äúenterprise‚Äù)
A) Attach the report to the incident thread
  send the S3 path in SNS email
  or publish to Slack via webhook (optional)

B) Add a ‚ÄúDeep report‚Äù mode
  Run 60-minute window queries
  Add top URIs, top IPs, block rate bins
  Add ALB metrics query (GetMetricData)

C) Add WAF redaction / filtering
AWS WAF supports redacted fields and filtering when enabling logging.
Documentation: https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html?utm_source=chatgpt.com













## LAB1C-Bonus-I Auto-IR Runbook
#### Deliverables
* [John Sweeney - Deliverables](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)


Human + Amazon Bedrock Incident Response

Purpose
This runbook defines how a human on-call engineer uses the Bedrock-generated incident report safely, verifies it against raw evidence, and produces a final, auditable incident artifact.

Core rule:
Bedrock accelerates analysis. Humans own correctness.






---

## [üåê LAB2]
#### Deliverables
* [John Sweeney LAB2 - DELIVERABLES](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations/tree/LAB2/LAB2-DELIVERABLES)
* [John Sweeney LAB2 - Code](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations)


### Repurpose

* Build on **Be A Man 1.1**
* Connect to the **running EC2 instance** via SSH
* Use **Vim** to edit `/var/www/html/index.html`
* Add additional deliverables (wife, yacht, motivation)

### Tasker 2

### HW - Be A Man Challenge 1.2 (10 pts)

* Must be in a GitHub repo link

* Modify the currently running web server from 1.1 to add:

  ```plaintext
  "I found my wife on a party yacht in <insert location here>! 
  Her name is <insert name here>!"
  ```

* Include a picture of the **woman you will have 5 sons by after making your cloud money**

---

## [üåê LAB 3]
[John Sweeney LAB3 - Deliverables](https://github.com/jastek69/Armageddon-C7-Lab3-SEIR_Foundations/tree/main/LAB3-DELIVERABLES)
[John Sweeney LAB3 - Code](https://github.com/jastek69/Armageddon-C7-Lab3-SEIR_Foundations/tree/main)


## üìù Summary

Through these challenges, you will learn how to:

* Launch and configure a **custom webserver** on AWS EC2
* Serve personalized content through Apache and HTML
* Connect to a live instance and update it in real time
* Troubleshoot AWS instances with security groups, Apache service, and logs
* Build confidence in maintaining a **live cloud service**
* Scale infrastructure using multi-region Terraform deployments
* Perform Git operations including listing, branching, and pull requests
* Practice Git merge workflows with pull requests and screenshots
* **Automate and secure infrastructure** using Terraform for multi-region design

üëâ Completing these tasks is the **next step in the Be A Man journey** toward mastering cloud automation.

---

## ‚úçÔ∏è Authors & Acknowledgments

* **Author and Group Leaders** T.I.Q.S., John Sweeney
* **Team Member:** 
