# ‚òÅÔ∏è Class 7 Armageddon - Brotherhood of Evil jerMutants - Wolfpack

![AWS](https://img.shields.io/badge/AWS-Cloud-orange?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Terraform](https://img.shields.io/badge/Terraform-%E2%89%A51.9-844FBA?style=for-the-badge&logo=terraform&logoColor=white)
![CloudFront](https://img.shields.io/badge/CloudFront-Edge_Security-yellow?style=for-the-badge&logo=amazon-aws)
![WAFv2](https://img.shields.io/badge/AWS_WAFv2-Real_Time_Logging-red?style=for-the-badge&logo=amazonaws)
![Bedrock](https://img.shields.io/badge/Amazon_Bedrock-Auto_IR-black?style=for-the-badge&logo=amazon-aws)
![Multi_Region](https://img.shields.io/badge/Multi_Region-Transit_Gateway-blue?style=for-the-badge)
![Compliance](https://img.shields.io/badge/Compliance-HIPAA_Inspired-purple?style=for-the-badge)
![Observability](https://img.shields.io/badge/Observability-CloudWatch_&_Bedrock-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Production_Grade-success?style=for-the-badge)

This repository contains links to repos of group members for multiple **Labs for Class 7 Armageddon (1a, 1b, 1c, 1d, 1e, 1f, 1g, 1h, 1i, 2a, 2b, 3a, 3b, and 4)**.  
Each LAB is tracked in its own branch with unique deliverables and tasks.

---

## GROUP MEMBER DELIVERABLES/REPOs

### **Group Co-Leader: T.I.Q.S**

| Lab                          | Deliverables                                                                                                  | Repo                                                                               |
|------------------------------|---------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **Lab 1A**                   | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1a/README.md)                        | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1a)         |
| **Lab 1B/1C**                | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1b-1c/README.md)                     | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1b-1c)      |
| **Lab 1C (Bonus A)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-a/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-a) |
| **Lab 1C (Bonus B)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-b/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-b) |
| **Lab 1C (Bonus C)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-c/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-c) |
| **Lab 1C (Bonus D)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-d/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-d) |
| **Lab 1C (Bonus E)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-e/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-e) |
| **Lab 1C (Bonus F)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-f/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-f) |
| **Lab 1C (Bonus G)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-g/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-g) |
| **Lab 1C (Bonus G)**         | [scripts-results](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-g/scripts-results) | N/A                                                                                |
| **Lab 1C (Bonus H)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-h/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-h) |
| **Lab 1C (Bonus H)**         | [deliverables](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-h/deliverables)       | N/A                                                                                |
| **Lab 1C (Bonus I)**         | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-1c-bonus-i/README.md)                | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-1c-bonus-i) |
| **Lab 2A**                   | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-2a/README.md)                        | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-2a)         |
| **Lab 2B**                   | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-2b/README.md)                        | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-2b)         |
| **Lab 2B (BAM Challenge A)** | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-2b-bam-a/README.md)                  | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-2b-bam-a)   |
| **Lab 2B (BAM Challenge B)** | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-2b-bam-b/README.md)                  | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-2b-bam-b)   |
| **Lab 2B (BAM Challenge C)** | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-2b-bam-c/README.md)                  | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-2b-bam-c)   |
| **Lab 3A**                   | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-3a/README.md)                        | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-3a)         |
| **Lab 3B**                   | [README](https://github.com/tiqsclass6/aws-armageddon-class-7.0/blob/lab-3b/README.md)                        | [Repo](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-3b)         |
| **Lab 3B**                   | [audit-pack](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-3b/audit-pack)                   | N/A                                                                                |
| **Lab 3B**                   | [deliverables](https://github.com/tiqsclass6/aws-armageddon-class-7.0/tree/lab-3b/deliverables)               | N/A                                                                                |

### **Group Co-Leader: John Sweeney**

| Lab  | Deliverables | Repo |
| ------------- |-------------|-------------:|
| Lab 1 A-B:    | [LAB 1 A-B DELIVERABLES](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1-DELIVERABLES) | N/A 
| Lab 1 C-I:     | [LAB 1 C-I DELIVERABLES](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1-DELIVERABLES)| N/A
| Lab 2 A-B:     |  [LAB 2 A-B - DELIVERABLES](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations/tree/LAB2/)     | [Lab 2 - Code](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations) 
| Lab 3 A-B   | [Lab 3 A-B DELIVERABLES](https://github.com/jastek69/Armageddon-C7-Lab3-SEIR_Foundations/tree/main/LAB3-DELIVERABLES) | [Lab 3 - Code](https://github.com/jastek69/Armageddon-C7-Lab3-SEIR_Foundations/tree/main)
| Lab 4    | [LAB 4 - DELIVERABLES] | [Lab 4 Code](https://github.com/jastek69/Armageddon-C7-LAB4-SEIR)


### **Member: Ernest Morris**

- Labs 1A-B: [Deliverables](https://github.com/jastek69/armageddon-C7-BHOEjM-Wolfpack.git)

### **Member: Michael Dale**

- Lab 1A through Lab 1C-Bonus-F: [Deliverables](https://github.com/MichaelDale1/Class-7-Armageddon-2026-Dale)

### **Member: Benji**

- Lab 1A through Lab 1C: [Deliverables](https://github.com/Newest-IT-Bro/Armageddon-repo.git)

### **Member: Jordan**

- Lab 1A-Lab 1B: [Deliverables](https://github.com/Jordan14Ford/Jordan-SEIR-Armageddon)

---

## üåê LAB 1

### LAB 1A-B

#### Deliverables

- [tiqsclass6 - Overview](https://github.com/tiqsclass6)
- [John Sweeney - Deliverable](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations/tree/LAB2/LAB1-DELIVERABLES)
- [Ernest Morris - Deliverables](https://github.com/jastek69/armageddon-C7-BHOEjM-Wolfpack.git)

### [Lab1a Explanation](https://github.com/BalericaAI/armageddon/blob/main/SEIR_Foundations/LAB1/1a_explanation.md?plain=1)

**Project Overview (What You Are Building)**  
In this lab, you will build a classic cloud application architecture:

- A compute layer running on an Amazon EC2 instance
- A managed relational database hosted on Amazon RDS
- Secure connectivity between the two using VPC networking and security groups
- Credential management using AWS Secrets Manager
- A simple application that writes and reads data from the database

### [Lab1b - explanation](https://github.com/BalericaAI/armageddon/blob/main/SEIR_Foundations/LAB1/1b_Incident_response.md?plain=1)

**Lab 1b ‚Äî Incident Response Scenario**  
Prerequisite: Lab 1a + Lab 1b infrastructure completed

#### **PART I ‚Äî Incident Scenario (What Messed Up)**

- **Incident Title**  
  Database Connectivity Failure ‚Äî Production Application Unavailable

- **Symptoms Reported**
  - Application intermittently returns errors
  - /list endpoint fails or hangs
  - No recent code changes
  - EC2 instance is still running

- **Constraints**
  - You may NOT:
    - Recreate EC2
    - Recreate RDS
    - Hardcode credentials

- **You MUST:**
  - Use logs
  - Use alarms
  - Use stored configuration values

#### **PART II ‚Äî Incident Injection (Group Leader / Auto-Grader)**

Choose one of the following to trigger the incident:

- **Option A ‚Äî Secret Drift (Recommended)**
  - Change DB password in Secrets Manager
  - Do not update the actual RDS password

- **Option B ‚Äî Network Isolation**
  - Remove EC2 security group from RDS inbound rule (TCP 3306)

- **Option C ‚Äî DB Interruption**
  - Stop RDS instance temporarily

Students are not told which failure was injected.

#### **PART III ‚Äî Monitoring & Alerting (SNS + PagerDuty Simulation)**

1. **SNS Alert Channel**
   - SNS Topic
   - Name: `lab-db-incidents`
   - `aws sns create-topic --name lab-db-incidents`
   - Email Subscription (PagerDuty Simulation)

     ```bash
     aws sns subscribe \
       --topic-arn <TOPIC_ARN> \
       --protocol email \
       --notification-endpoint your-email@example.com
     ```

   This simulates PagerDuty / OpsGenie paging an engineer.

2. **CloudWatch Alarm ‚Üí SNS**  
   **Alarm Concept**  
   Trigger when: DB connection errors ‚â• 3 in 5 minutes

   **Alarm Creation (example)**

   ```bash
   aws cloudwatch put-metric-alarm \
     --alarm-name lab-db-connection-failure \
     --metric-name DBConnectionErrors \
     --namespace Lab/RDSApp \
     --statistic Sum \
     --period 300 \
     --threshold 3 \
     --comparison-operator GreaterThanOrEqualToThreshold \
     --evaluation-periods 1 \
     --alarm-actions <SNS_TOPIC_ARN>
   ```

   **Expected Behavior**  
   - Alarm transitions to ALARM  
   - SNS notification sent  
   - Student receives alert email

#### **PART IV ‚Äî Mandatory Incident Runbook**

Students must follow this order. Deviations lose points.

##### **RUNBOOK SECTION 1 ‚Äî Acknowledge**

1.1 Confirm Alert

```bash
aws cloudwatch describe-alarms \
  --alarm-name lab-db-connection-failure \
  --query "MetricAlarms[].StateValue"
```

Expected: `ALARM`

##### **RUNBOOK SECTION 2 ‚Äî Observe**

2.1 Check Application Logs

```bash
aws logs filter-log-events \
  --log-group-name /aws/ec2/lab-rds-app \
  --filter-pattern "ERROR"
```

Expected: Clear DB connection failure messages

2.2 Identify Failure Type  
Students must classify:

- Credential failure?
- Network failure?
- Database availability failure?

This classification is graded.

##### **RUNBOOK SECTION 3 ‚Äî Validate Configuration Sources**

3.1 Retrieve Parameter Store Values

```bash
aws ssm get-parameters \
  --names /lab/db/endpoint /lab/db/port /lab/db/name \
  --with-decryption
```

Expected: Endpoint + port returned

3.2 Retrieve Secrets Manager Values

```bash
aws secretsmanager get-secret-value \
  --secret-id lab/rds/mysql
```

Expected: Username/password visible  
Compare against known-good state

##### **RUNBOOK SECTION 4 ‚Äî Containment**

4.1 Prevent Further Damage

- Do not restart EC2 blindly
- Do not rotate secrets again
- Do not redeploy infrastructure

Students must explicitly state:  
‚ÄúSystem state preserved for recovery.‚Äù

##### **RUNBOOK SECTION 5 ‚Äî Recovery**

Recovery Paths (Depends on Root Cause)

- If Credential Drift
  - Update RDS password to match Secrets Manager  
    OR
  - Update Secrets Manager to known-good value

- If Network Block
  - Restore EC2 security group access to RDS on 3306

- If DB Stopped
  - Start RDS and wait for available

##### **Verify Recovery**

```bash
curl http://<EC2_PUBLIC_IP>/list
```

Expected: Application returns data, no errors

##### **RUNBOOK SECTION 6 ‚Äî Post-Incident Validation**

6.1 Confirm Alarm Clears

```bash
aws cloudwatch describe-alarms \
  --alarm-name lab-db-connection-failure \
  --query "MetricAlarms[].StateValue"
```

Expected: `OK`

6.2 Confirm Logs Normalize

```bash
aws logs filter-log-events \
  --log-group-name /aws/ec2/lab-rds-app \
  --filter-pattern "ERROR"
```

Expected: No new errors

#### **PART V ‚Äî Grading Rubric (100 Points)**

| Category                          | Points |
|-----------------------------------|--------|
| Alarm acknowledged via CLI        | 10     |
| Correct failure classification    | 20     |
| Logs used correctly               | 15     |
| Parameter Store validated         | 10     |
| Secrets Manager validated         | 10     |
| Correct recovery action           | 20     |
| No redeploy / no guesswork        | 10     |
| Clear incident summary            | 5      |

#### **PART VI ‚Äî Required Incident Report (Short)**

Students must submit:

- Incident Summary
  - What failed?
  - How was it detected?
  - Root cause
  - Time to recovery

- Preventive Action
  - One improvement to reduce MTTR
  - One improvement to prevent recurrence

#### **PART VII ‚Äî What This Actually Teaches**

By completing this lab, students demonstrate:

- On-call discipline
- Root cause analysis
- Cloud-native recovery
- Proper secret handling
- Operational maturity

This is exactly what separates:  
‚ÄúI passed an AWS cert‚Äù  
from  
‚ÄúI can be trusted with production.‚Äù

## Lab 1C ‚Äî Terraform: EC2 ‚Üí RDS + Secrets/Params + Observability + Incident Alerts

### LAB 1C-I

#### Deliverables For LAB C-I

- [tiqsclass6 - Overview](https://github.com/tiqsclass6)
- [John Sweeney - Deliverables 1 C-I](https://github.com/jastek69/armageddon-C7-LAB1C-H/tree/main/LAB1C-H-DELIVERABLES)

### Purpose

Modern companies do not build AWS by clicking around in the console.  
They use Infrastructure as Code (IaC) so environments are repeatable, reviewable, auditable, and recoverable.

This repo is intentionally incomplete:

- It declares required resources
- Students must configure the details (rules, policies, user_data, app logging, etc.)

### Requirements (must exist in Terraform)

- VPC, public/private subnets, IGW, NAT, routing
- EC2 app host + IAM role/profile
- RDS (private) + subnet group + SG with inbound from EC2 SG
- Parameter Store values (`/lab/db/*`)
- Secrets Manager secret (db creds)
- CloudWatch log group
- CloudWatch alarm (DBConnectionErrors >= 3 per 5 min)
- SNS topic + subscription

### Student Deliverables

- `terraform plan` output
- `terraform apply` evidence (outputs)
- CLI verification commands (from Lab 1b)
- Incident runbook execution notes (alarm fired + recovered)

## üåê LAB 2
#### Deliverables
[John Sweeney: LAB 2 A-B - DELIVERABLES](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations/tree/LAB2/)     | [Lab 2 - Code](https://github.com/jastek69/Armageddon-C7-SEIR_Foundations)

### Lab 2A = ‚ÄúOrigin Cloaking + CloudFront as the only public ingress.‚Äù
The clean, realistic interpretation of your requirement (and what big orgs actually do) is:
Only CloudFront is publicly reachable. ALB is still ‚Äúinternet-facing‚Äù (because CloudFront must reach it), but it‚Äôs cloaked so direct access is blocked: Security Group allows inbound only from the AWS-managed CloudFront origin-facing prefix list (com.amazonaws.global.cloudfront.origin-facing). ALB listener requires a secret custom header that only CloudFront adds. WAF moves to CloudFront (WAFv2 scope = "CLOUDFRONT"), and it is associated to the distribution. chewbacca-growl.com (and app.chewbacca-growl.com) alias to CloudFront.

### LAB 2B: stop ‚Äúusing CloudFront‚Äù and start operating CloudFront correctly.

The entire lab is built around one idea AWS emphasizes: cache key (cache policy) and origin forwarding (origin request policy) are different knobs, and getting them wrong causes real incidents (user A sees user B‚Äôs data, auth breaks, ‚Äúrandom 403s,‚Äù etc.). https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/understanding-how-origin-request-policies-and-cache-policies-work-together.html?utm_source=chatgpt.com

Below is a full Lab 2B package: project intro + workforce relevance expected deliverables Terraform overlay (Chewbacca style, skeleton where students fill values) correctness tests (curl + headers + ‚Äúcache poisoning‚Äù checks)







## üåê LAB 3
#### Deliverables
 [John Sweeney: Lab 3 A-B DELIVERABLES](https://github.com/jastek69/Armageddon-C7-Lab3-SEIR_Foundations/tree/main/LAB3-DELIVERABLES) | [Lab 3 - Code](https://github.com/jastek69/Armageddon-C7-Lab3-SEIR_Foundations/tree/main)


#### Lab 3 ‚Äî Japan Medical
Cross-Region Architecture with Legal Data Residency (APPI Compliance)
Scenario Overview
  A Japanese medical organization operates:
  A primary medical data system in Tokyo
  A satellite medical office in S√£o Paulo
  A single global application URL: chewbacca-growls.com
  Global access via CloudFront
  Strict legal requirement:
    All Japanese patient medical data (PHI) must remain physically stored in Japan

This is not a theoretical exercise.
This is how regulated global healthcare systems are actually built.

Why This Lab Exists (Read This Carefully)
Japan‚Äôs privacy law ‚Äî ÂÄã‰∫∫ÊÉÖÂ†±‰øùË≠∑Ê≥ï (APPI) ‚Äî requires that personally identifiable medical information for Japanese citizens must not be stored outside Japan, unless extremely specific legal mechanisms are in place.

For healthcare:
  The safe, standard interpretation is:
    Store PHI only inside Japan

Even if:
    The patient is traveling
    The doctor is overseas
    The application is globally accessible

üìå Access is allowed. Storage is not.
That single sentence is the key mental shift.

Legal Reality ‚Üí Architectural Consequence

Because of APPI:
| Component             | Allowed Location                           |
| --------------------- | ------------------------------------------ |
| RDS (Medical Records) | **Tokyo only** (`ap-northeast-1`)          |
| Backups / snapshots   | **Tokyo only**                             |
| Read replicas         | ‚ùå Not allowed outside Japan                |
| App access            | ‚úÖ Allowed globally                         |
| CloudFront            | ‚úÖ Allowed (edge cache, no PHI persistence) |
| EC2 in S√£o Paulo      | ‚úÖ Allowed (stateless compute only)         |

This forces a hub-and-spoke architecture:
  Tokyo is the data authority
  Other regions are compute-only extensions

Regional Architecture Breakdown
üáØüáµ Tokyo ‚Äî Primary Region (ap-northeast-1)
Tokyo hosts everything that touches patient data at rest:
    RDS (MySQL / PostgreSQL)
    Primary VPC
    Application tier (EC2 / ASG)
    Transit Gateway attachment
    Parameter Store & Secrets Manager (authoritative)
    Logging, auditing, backups

Tokyo is the single source of truth.
If Tokyo goes down:
    The system degrades
    But data residency is never violated

This is intentional.

üáßüá∑ S√£o Paulo ‚Äî Satellite Region (sa-east-1)
S√£o Paulo exists only to improve access latency for doctors and staff physically located there.
S√£o Paulo contains:
    VPC
    EC2 + Auto Scaling Group
    No databases
    No local persistence of PHI
    No backups
    No replicas

Every read/write:
    Traverses the AWS backbone
    Goes directly to Tokyo RDS
    Is encrypted in transit
    Is logged and auditable
S√£o Paulo is stateless compute.

Why Transit Gateway Is Used (Not VPC Peering)
At this scale and sensitivity:
    VPC peering becomes brittle
    Routing rules multiply
    Auditing cross-region flows becomes harder
Transit Gateway provides:
    Centralized routing
    Explicit control of allowed paths
    Clear inspection points
    Enterprise-grade segmentation
In regulated environments:
    Transit Gateway is preferred because it creates a visible, controllable data corridor.

That matters for audits.

CloudFront‚Äôs Role (Single URL, Multiple Regions)
  There is only one public URL: https://chewbacca-growls.com

CloudFront:
    Terminates TLS
    Applies WAF
    Routes users to the nearest healthy region
    Never stores PHI
    Only caches:
        Static assets
        Non-sensitive responses
        Content explicitly marked cacheable

  CloudFront is legally safe because:
    It is not a data stor
    It does not persist medical records
    It respects cache control rules

Data Flow (End-to-End)

Let‚Äôs walk a real example.

Example: Japanese patient visiting S√£o Paulo
    1. Patient visits clinic in S√£o Paulo
    2. Doctor opens chewbacca-growls.com
    3. CloudFront routes request to S√£o Paulo EC2
    4. S√£o Paulo EC2:
        Authenticates request
        Does not store PHI locally
        Opens encrypted connection to Tokyo RDS via Transit Gateway
    5. Data is read/written only in Tokyo
    6. Response is returned to S√£o Paulo doctor

At no point:
    Is PHI stored outside Japan
    Is data replicated
    Is a local database created

This satisfies APPI compliance.

Why This Is the Correct Tradeoff
What Japan cares about
    Data sovereignty
    Auditability
    Legal certainty

What the business cares about
    Doctors can work where patients are
    Latency is reasonable
    Single global app
    No duplicated systems

This architecture:
    Accepts slightly higher latency
    In exchange for legal compliance and operational simplicity

That is the correct trade in healthcare.

What Would Be Illegal (And Why)

‚ùå RDS Read Replica in S√£o Paulo
‚Üí Data at rest outside Japan

‚ùå Aurora Global Database
‚Üí Storage replication outside Japan

‚ùå Local cache of patient records on EC2 disk
‚Üí Persistent PHI outside Japan

‚ùå CloudFront caching PHI
‚Üí Edge persistence outside Japan

These are not ‚Äúbad practices.‚Äù
They are compliance violations.

Why This Lab Matters for Your Career
  Most engineers:
    Learn ‚Äúmulti-region for availability‚Äù
    Learn ‚Äúreplicate everything everywhere‚Äù

Regulated reality is different.
This lab teaches you to:
    Translate law into architecture
    Design global systems with asymmetric constraints
    Explain why a slower design is the correct one
    Speak confidently to:
      Security
      Legal
      Compliance
      Auditors

If you can explain this architecture clearly, you are senior-level.

How to Talk About This in an Interview

    ‚ÄúI designed a multi-region medical application where all PHI remained in Japan to comply with APPI.
    CloudFront provided global access, S√£o Paulo ran stateless compute only, and all reads/writes traversed a Transit Gateway to Tokyo RDS.
    The design intentionally traded some latency for legal certainty and auditability.‚Äù

That answer will stop the room.

One Sentence to Remember
  ---> Global access does not require global storage.

That sentence is the heart of modern regulated cloud architecture.


### LAB 3A
Lab 3A ‚Äî Japan Medical
Cross-Region Architecture with Transit Gateway (APPI-Compliant)

üéØ Lab Objective
In this lab, you will design and deploy a cross-region medical application architecture that:
  Uses two AWS regions
    Tokyo (ap-northeast-1) ‚Äî data authority
    S√£o Paulo (sa-east-1) ‚Äî compute extension
  Connects regions using AWS Transit Gateway
  Serves traffic through a single global URL
  Stores all patient medical data (PHI) only in Japan
  Allows doctors overseas to read/write records legally

This lab is a warm-up for real DevOps and platform engineering, where:
  environments are separated
  Terraform state is split
  pipelines are independent
  coordination matters more than copy-paste

üè• Real-World Context (Why This Exists)

Japan‚Äôs privacy law, ÂÄã‰∫∫ÊÉÖÂ†±‰øùË≠∑Ê≥ï (APPI), places strict requirements on the handling of personal and medical data.
For healthcare systems, the safest and most common interpretation is:
    Japanese patient medical data must be stored physically inside Japan. (Don't even mess with this)

This applies even when:
    the patient is traveling abroad
    the doctor is located overseas
    the application is accessed globally

üìå Access is allowed. Storage is not.
    --> This lab models how real medical systems comply with that rule.

üåç Regional Roles
üáØüáµ Tokyo ‚Äî Primary Region (Data Authority)
Tokyo is the source of truth.
It contains:
    RDS (medical records)
    Primary VPC
    Application tier (Lab 2 stack)
    Transit Gateway (hub)
    Parameter Store & Secrets Manager (authoritative)
    Logging, auditing, backups
    Really hot chicks who need men to impregnate them. 

All data at rest lives here.
If Tokyo is unavailable:
    the system may degrade
    but data residency is never violated

This is intentional and correct.

üáßüá∑ S√£o Paulo ‚Äî Secondary Region (Compute-Only)

S√£o Paulo exists to serve doctors and staff physically located in South America.

It contains:
    VPC
    EC2 + Auto Scaling Group
    Application tier (Lab 2 stack)
    Transit Gateway (spoke)
    Even hotter chicks who need you to throw it down and impregnate them.

It does not contain:
    RDS
    Read replicas
    Backups
    Persistent storage of PHI
    Keisha. No Keisha here.

S√£o Paulo is stateless compute.<----> All reads and writes go directly to Tokyo.

üåê Networking Model
Why Transit Gateway?
Transit Gateway is used instead of VPC peering because it provides:
    Clear, auditable traffic paths
    Centralized routing control
    Enterprise-grade segmentation
    A visible ‚Äúdata corridor‚Äù for compliance reviews

In regulated environments, clarity beats convenience.

How Traffic Flows

Doctor (S√£o Paulo)
   ‚Üì
CloudFront (global edge)
   ‚Üì
S√£o Paulo EC2 (stateless)
   ‚Üì
Transit Gateway (S√£o Paulo)
   ‚Üì
TGW Peering
   ‚Üì
Transit Gateway (Tokyo)
   ‚Üì
Tokyo VPC
   ‚Üì
Tokyo RDS (PHI stored here only)
The entire path stays on the AWS backbone and is encrypted in transit.

üåê Single Global URL

There is only one public URL: https://chewbacca-growls.com

CloudFront:
    Terminates TLS
    Applies WAF
    Routes users to the nearest healthy region
    Never stores patient data
    Caches only content explicitly marked safe

CloudFront is allowed because:
    it is not a database
    it does not persist PHI
    it respects cache-control rules

üèóÔ∏è Terraform & DevOps Structure
Important: Multi-Terraform-State Reality

In real organizations, regions are not deployed from one Terraform state.

For this lab:
    Tokyo and S√£o Paulo are separate Terraform states
    Each state will eventually map to a separate Jenkins job
    States communicate only through:
        Terraform outputs
        Remote state references
        Explicit variables

This is intentional.---> You are learning how real DevOps teams coordinate infrastructure.

Expected Repository Layout
lab-3/
‚îú‚îÄ‚îÄ tokyo/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf        # Lab 2 + marginal TGW hub code
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf     # Exposes TGW ID, VPC CIDR, RDS endpoint
‚îÇ   ‚îî‚îÄ‚îÄ variables.tf
‚îÇ
‚îú‚îÄ‚îÄ saopaulo/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf        # Lab 2 minus DB + TGW spoke code
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îî‚îÄ‚îÄ data.tf        # Reads Tokyo remote state

üöÜ Naming Conventions (Important)

To make the architecture feel local and intentional:
Tokyo (train stations)
    shinjuku-*
    shibuya-*
    ueno-*
    akihabara-*

S√£o Paulo (Japanese district)
    liberdade-*

You should be able to look at a resource name and know the region immediately.

üîß What Changes from Lab 2
Tokyo (minimal changes)
    Add Transit Gateway
    Attach Tokyo VPC to TGW
    Create TGW peering request
    Add return routes for S√£o Paulo CIDR
    Update RDS security group to allow S√£o Paulo VPC CIDR

S√£o Paulo (new deployment)
    Deploy Lab 2 stack without RDS
    Create S√£o Paulo Transit Gateway
    Accept TGW peering
    Attach S√£o Paulo VPC to TGW
    Add routes pointing Tokyo CIDR ‚Üí TGW

üîê Security Model (Read Carefully)
  RDS allows inbound only from:
    Tokyo application subnets
    S√£o Paulo VPC CIDR (explicitly)
  No public DB access
  No local PHI storage in S√£o Paulo
  All access is logged and auditable

This is compliance by design, not by policy.

‚úÖ What You Must Prove (Verification)
From a S√£o Paulo EC2 instance:
    You can connect to Tokyo RDS
    The application can read/write records
    No database exists in S√£o Paulo

From the AWS console / CLI:
    TGW attachments exist in both regions
    Route tables contain cross-region CIDRs
    Traffic flows only through TGW

‚ùå What Is Explicitly Not Allowed
    RDS outside Tokyo
    Cross-region replicas
    Aurora Global Database
    Local caching of patient records
    CloudFront caching PHI
    ‚ÄúActive/active‚Äù databases

If you do these, the architecture is illegal, not just ‚Äúwrong‚Äù.

üéì Why This Lab Matters for Your Career

Most engineers learn:
  ‚ÄúMake it multi-region‚Äù
  ‚ÄúReplicate everything‚Äù
  "Study CompTia and give my money to Keisha"{

This lab teaches you:
  How law shapes architecture
  How to design asymmetric global systems
  How to explain tradeoffs to security, legal, and auditors
  How DevOps actually works across teams and states
  Become a Passport Bro and marry the girl of your dreams

If you can explain this lab clearly, you are operating at a Senior level.

üó£Ô∏è Interview Talk Track (Memorize This)

    ‚ÄúI designed a cross-region medical system where all PHI remained in Japan to comply with APPI.
    Tokyo hosted the database, S√£o Paulo ran stateless compute, and Transit Gateway provided a controlled data corridor.
    CloudFront delivered a single global URL without violating data residency.‚Äù

That answer will stop the room.

üß† One Sentence to Remember---> Global access does not require global storage.
    Anothe Sentence to Remember ---> I completed this lab in 2026 and now in 2029, I have a family.





### LAB 3B
Deliverables (Students must submit)
Deliverable A ‚Äî ‚ÄúAudit Evidence Pack‚Äù (one folder)

Â≠¶Áîü„ÅØ audit-pack/ „Éï„Ç©„É´„ÉÄ„ÇíÊèêÂá∫„ÄÇ

audit-pack/
‚îú‚îÄ‚îÄ 00_architecture-summary.md
‚îú‚îÄ‚îÄ 01_data-residency-proof.txt
‚îú‚îÄ‚îÄ 02_edge-proof-cloudfront.txt
‚îú‚îÄ‚îÄ 03_waf-proof.txt
‚îú‚îÄ‚îÄ 04_cloudtrail-change-proof.txt
‚îú‚îÄ‚îÄ 05_network-corridor-proof.txt
‚îî‚îÄ‚îÄ evidence.json   (Malgus scripts output)

Deliverable B ‚Äî One paragraph ‚Äúauditor narrative‚Äù
‚Äú„Åì„ÅÆË®≠Ë®à„Åå APPI ÁöÑ„Å´ÂÆâÂÖ®„Åß„ÄÅ„Å™„Åú DB „ÇíÊµ∑Â§ñ„Å´ÁΩÆ„Åë„Å™„ÅÑ„Åã‚Äù„Çí 8„Äú12 Ë°å„ÅßË™¨Êòé„ÄÇ

Verification Commands (CLI proof students can paste)
1) Data residency proof (RDS only in Tokyo)

    Tokyo: RDS exists

            aws rds describe-db-instances --region ap-northeast-1 \
          --query "DBInstances[].{DB:DBInstanceIdentifier,AZ:AvailabilityZone,Region:'ap-northeast-1',Endpoint:Endpoint.Address}"

    S√£o Paulo: No RDS

            aws rds describe-db-instances --region sa-east-1 \
          --query "DBInstances[].DBInstanceIdentifier"


2) Edge proof (CloudFront logs show cache + access)
    Students capture request headers:

        curl -I https://chewbacca-growls.com/api/public-feed

And/or submit CloudFront standard log evidence (Hit/Miss/RefreshHit)

3) WAF proof

Provide:
        WAF log snippet or Insights summary
        WAF logging destination options are documented 


4) Change proof (CloudTrail)
CloudTrail has event history with a 90-day immutable record of management events 

Students capture:
        --> ‚Äúwho changed SG / TGW route / WAF / CloudFront config‚Äù

5) Network corridor proof (TGW)
Students prove:
        TGW attachments exist in both regions
        routes point cross-region CIDRs to TGW

6) AWS CLI verification (students can prove the bucket/logs exist)

        aws s3 ls s3://Class_Lab3/
        # If logs are under a folder/prefix:
        aws s3 ls s3://Class_Lab3/cloudfront-logs/ --recursive | tail -n 20

Download one file manually (sanity check):

    aws s3 cp s3://Class_Lab3/cloudfront-logs/<somefile>.gz .

Script 1 ‚Äî malgus_residency_proof.py
Creates a ‚ÄúDB only in Tokyo‚Äù proof file.

Script 2 ‚Äî malgus_tgw_corridor_proof.py
Shows TGW attachments + routes that form the ‚Äúlegal corridor‚Äù.

Script 3 ‚Äî malgus_cloudtrail_last_changes.py
Pulls recent CloudTrail events for ‚Äúwho changed what‚Äù.
        --> Event history is available by default; it provides a 90-day record of management events.

Script 4 ‚Äî malgus_waf_summary.py
Summarizes WAF logs (Allow vs Block) from CloudWatch Logs destination.
WAF logging destinations: CloudWatch Logs, S3, Firehose.

Script 5 ‚Äî malgus_cloudfront_log_explainer.py (optional)
If you ingest CloudFront standard logs into S3, this script reads a log file and counts Hit/Miss/RefreshHit.

CloudFront standard logs reference Hit / RefreshHit semantics. 
A) Standard logs in S3 (downloaded locally)

        python3 malgus_cloudfront_log_explainer.py --mode standard cloudfront.log.gz
        python3 malgus_cloudfront_log_explainer.py --mode standard cloudfront_part1.log cloudfront_part2.log

B) Real-time logs as JSON lines

        python3 malgus_cloudfront_log_explainer.py --mode realtime realtime_logs.jsonl

Final Lab Assumptions (Locked)
    S3 Bucket: Class_Lab3
    CloudFront Logs Prefix: Chwebacca-logs/ ‚Üê intentionally misspelled
    AWS Account ID: 200819971986

Running Scripts:

        python3 malgus_cloudfront_log_explainer.py --latest 5
        python3 malgus_cloudfront_log_explainer.py --prefix cloudfront-logs/ --latest 10
        python3 malgus_cloudfront_log_explainer.py --prefix cloudfront-logs/ --latest 5 --keep


From stdin (nice for pipelines)

        zcat cloudfront.log.gz | python3 malgus_cloudfront_log_explainer.py --mode standard -

Where ‚ÄúHit / Miss / RefreshHit‚Äù come from (student-facing truth)
    In standard CloudFront logs, you usually read the field:
        x-edge-result-type (primary)
        sometimes also x-edge-response-result-type

    Values commonly include: Hit, Miss, RefreshHit, plus other states like Error, LimitExceeded, etc.

That‚Äôs why the script reports ‚ÄúOther:*‚Äù ‚Äî so students don‚Äôt blindly ignore unusual outcomes.



        









## üåê LAB 4
#### Deliverables
 [John Sweeney: LAB 4 - DELIVERABLES] | [Lab 4 Code](https://github.com/jastek69/Armageddon-C7-LAB4-SEIR)



#### Lab 4 ‚Äî Japan Medical
Multi-Cloud Reality in Regulated Healthcare

‚öñÔ∏è Why This Lab Exists
Many engineers fail in real life because:
    They optimize locally
    They assume uniform platforms
    They treat compliance as ‚Äúsomeone else‚Äôs problem‚Äù

This lab teaches:
    Cross-cloud thinking
    Legal translation into architecture
    Responsibility beyond your own stack
    Professional restraint

üéØ What This Lab Is About

In this lab, you are not solving a technical problem first.
You are solving a human, legal, and organizational problem.

A Japanese medical center operates globally.
    Tokyo is the data authority
    Compliance is non-negotiable
    And now:
      The New York branch refuses to use AWS.

Instead, they deploy on Google Cloud Platform (GCP).
No negotiations. No exceptions.
Your job is not to convince them otherwise.
Your job is to make the system work ‚Äî legally and responsibly.

üß† The First Reality: Technology Is Not the Decision
Many engineers assume:
    ‚ÄúIf we pick the right platform, the problem goes away.‚Äù

In real organizations:
    Technology choices are political
    Vendor preferences exist
    Contracts predate architecture
    Teams have autonomy
    Mergers create fragmentation

You will encounter:
    AWS in one region
    GCP in another
    Azure somewhere else
    Oracle, IBM, OpenShift, or on-prem in legacy branches

üìå Compliance does not change just because technology does.


üè• Legal Constraint Still Applies (This Does Not Change)
Even in a multi-cloud world:
    --> Japanese patient medical data (PHI) must be stored only in Japan.

This rule does not bend for:
    GCP
    Azure
    ‚ÄúBetter latency‚Äù
    ‚ÄúLocal autonomy‚Äù
    ‚ÄúIt‚Äôs inconvenient‚Äù

There is:
    No exemption
    No workaround
    No ‚Äútemporary‚Äù exception


üåé What the New York Branch Is Allowed to Do
The New York branch (on GCP) may:
    Deploy compute only (VMs, autoscaling groups)
    Serve doctors locally
    Authenticate users
    Process requests in memory
    Call APIs across providers
    Read and write data remotely

The New York branch may not:
    Store patient data at rest
    Deploy databases
    Cache medical records
    Replicate data
    Snapshot, export, or log PHI

This is exactly the same rule as S√£o Paulo ‚Äî the platform changed, the law did not.

üîó Connectivity Must Respect Compliance
You are now operating across:
    Cloud providers
    Legal jurisdictions
    Organizational boundaries

The system must ensure:
    Secure connectivity from GCP ‚Üí Japan
    Clear network paths
    Encryption in transit
    Strong identity and access controls
    Complete auditability

And critically:
    --> No accidental data persistence outside Japan

This includes:
    Disk
    Logs
    Queues
    Caches
    Backups
    Temporary files

üßë‚Äç‚öïÔ∏è Focus on the Human Experience
    This lab is not just infrastructure.
    You must consider three people:

üë©‚Äç‚öïÔ∏è Doctor (New York)
Expectations:
    Fast, reliable access to patient records
    No concern about where data lives
    No manual compliance steps
    Trust that the system is legal

Risks:
    Latency
    Connectivity failures
    Partial outages

Your responsibility:
    --> Design systems where doctors never have to think about compliance ‚Äî because you already did.


üßë‚Äçü¶Ω Patient (Japanese Citizen)
Expectations:
    Their data is protected
    Their data is not exported
    Their rights are respected
    Their records are accurate

Patients do not care about:
    AWS vs GCP
    Cloud vendors
    Network topology

They care about:
    --> Trust

Your responsibility:
    --> Architect in a way that never betrays that trust.

üßë‚Äçüíº Manager / Executive
Expectations:
    Branch autonomy
    Regulatory safety
    No headlines
    No fines
    No ‚Äúwhy didn‚Äôt you tell us?‚Äù

Managers expect engineers to:
    Anticipate risk
    Explain tradeoffs clearly
    Say ‚Äúno‚Äù when required
    Provide defensible designs

Your responsibility:
    --> Make compliance boring and invisible.

üß† The Core Lesson of Lab 4
    You do not control the technology landscape.
    You control how responsibly it is connected.

Multi-cloud is not a badge of honor.
It is a constraint.

Good engineers complain.
Great engineers adapt without breaking the law.


üó£Ô∏è How You Should Talk About This Lab
  If asked in an interview:
      ‚ÄúWe supported a medical branch on GCP while keeping all PHI in Japan.
      The branch ran stateless compute only, and all patient data was accessed remotely under strict controls.
      Compliance dictated the architecture ‚Äî not cloud preference.‚Äù

That answer signals maturity.

### LAB 4 A Deliverables
Lab 4A ‚Äî Japan Medical
AWS ‚Üî GCP Secure Connectivity (IPSec VPN + BGP)

üéØ Lab Objective
In this lab, you will design and validate secure, compliant connectivity between:
    AWS Tokyo (ap-northeast-1) ‚Äî authoritative data region
    GCP Iowa (us-central1) ‚Äî New York branch compute (GCP side)

using:
    IPSec VPN
    BGP for dynamic routing
    No databases outside Japan
    Strict process discipline

This lab focuses on how regulated organizations connect clouds safely, not on ‚Äúfastest possible networking.‚Äù

üß† Why This Lab Exists (Reality Check)
In real enterprises:
    Not all branches use the same cloud
    Not all teams report to the same platform group
    Legal obligations override technical preference
    Connectivity requires coordination, not heroics

This lab simulates a real multi-team, multi-provider connection where mistakes have consequences.

üåç Architecture Overview
Regions
    AWS: Tokyo (ap-northeast-1)
    GCP: Iowa (us-central1)

Roles
  AWS Tokyo
    RDS (PHI lives here only)
    Transit Gateway (hub)
    VPN termination (TGW VPN attachment)

  GCP Iowa
    VMs only (no databases)
    Network Connectivity Center (NCC)
    Cloud VPN (HA VPN)
    Cloud Router (BGP)

üîó Connectivity Model
Technology Used
    IPSec VPN (site-to-site)
    BGP for dynamic route exchange
    Private IP routing only
    Encrypted in transit

BGP Link-Local Ranges (Given)
You must use the following BGP IP ranges:
  169.254.12.0/30
  169.254.12.4/30

These are link-local and used only inside the tunnel.

üîê Security & Compliance Constraints (Non-Negotiable)
    No PHI stored in GCP
    No databases in GCP
    No disk persistence of medical data
    No logging of PHI
    All traffic encrypted
    All routing explicit and auditable

If a solution ‚Äúworks‚Äù but violates these rules, it fails the lab.

üß© What Each Side Is Responsible For
AWS Tokyo Team (You)
    Create Transit Gateway VPN attachment
    Configure BGP ASN and peers
    Advertise only Tokyo VPC CIDR
    Accept routes from GCP
    Ensure return routing is correct
    Keep PHI inside Japan

GCP Iowa Team (You)
    Create HA VPN gateway
    Create Cloud Router (BGP enabled)
    Integrate with Network Connectivity Center
    Configure BGP peers with AWS
    Advertise only GCP VPC CIDR
    Ensure no data persistence

üîë Process Reminder: Pre-Shared Keys (PSKs)
This Is Important
In real environments:
    --> Pre-Shared Keys are never sent over chat, screenshots, or tickets.

For this lab:
    Each VPN tunnel uses a PSK
    PSKs must be:
      Generated securely
      Stored temporarily
      Shared out-of-band

Acceptable Methods (Lab Simulation)
    Password manager entry
    Secure note shared verbally
    Encrypted file (temporary)
    One-time secure message

Not Acceptable
    Slack / Teams plaintext
    Email
    Hard-coding in Terraform repos
    Screenshots

üìå Process discipline is part of the grade.

üîß High-Level Build Steps (No Code Yet)
Step 1 ‚Äî AWS Tokyo
    Confirm Transit Gateway exists
    Create Site-to-Site VPN attached to TGW
    Enable dynamic routing (BGP)
    Configure:
      Local ASN (AWS side)
      BGP peer IPs (169.254.12.x)
      Two tunnels for HA
    Export:
      Tunnel outside IPs
      BGP inside IPs
      ASN
      PSKs

Step 2 ‚Äî GCP Iowa
    Create HA VPN gateway
    Create Cloud Router
    Enable BGP
    Configure:
      Peer ASN
      Link-local IPs
      Matching PSKs
    Attach VPN to Network Connectivity Center
    Advertise GCP VPC CIDR

Step 3 ‚Äî Routing Validation
    Confirm:
      AWS sees GCP routes
      GCP sees Tokyo routes
      No extra CIDRs exchanged

‚úÖ What Students Must Prove (Deliverables)
üì¶ Deliverable 1 ‚Äî Connectivity Evidence
Submit screenshots or CLI output showing:
    BGP session Established
    Routes learned on both sides
    Only approved CIDRs exchanged

üì¶ Deliverable 2 ‚Äî Network Diagram
A simple diagram showing:
    AWS TGW
    VPN tunnels
    GCP HA VPN
    Cloud Router
    CIDR flow direction

Clarity > artwork.

üì¶ Deliverable 3 ‚Äî Process Write-Up (Short)
Answer in 6‚Äì10 sentences:
    How PSKs were generated
    How they were shared
    Why this matters in regulated environments

üì¶ Deliverable 4 ‚Äî Compliance Statement
One paragraph explaining:
    Why no data is stored in GCP
    Why this satisfies Japanese privacy requirements
    Why ‚Äúmulti-cloud‚Äù does not mean ‚Äúmulti-storage‚Äù

üéì Why This Lab Matters for Your Career
This lab teaches you to:
    Work across providers
    Coordinate between teams
    Respect legal boundaries
    Treat networking as shared responsibility
    Value process as much as code

Many engineers can configure VPNs.
Few can do it cleanly, safely, and defensibly.
      
üó£Ô∏è Interview Talk Track (Practice This)

  ‚ÄúWe connected a GCP-based medical branch to AWS using IPSec VPN and BGP, while ensuring all PHI remained in Japan.
  The GCP side ran compute only, and routing was tightly controlled to meet compliance requirements.‚Äù

That answer signals real-world readiness.
        
üß† One Sentence to Remember
Secure connectivity is as much about process as it is about packets.


What you‚Äôre getting below (GCP / Iowa us-central1):
        VPC + subnet (placeholders for 10.x.x.x/xx)
        Firewall rules so the HTTPS URL is ONLY reachable from VPN/TGW CIDRs
        Managed Instance Group (MIG) behind an Internal HTTPS Load Balancer (so it‚Äôs only reachable over VPN)
        Self-signed cert generated at boot on each VM (simple)
        Startup script that:
                installs Nginx + Python
                hosts a tiny internal app endpoint
                includes a basic ‚Äúwrite/read to Tokyo RDS‚Äù Python test script (students wire the DB params)
            Optional: Cloud NAT (so instances can apt-get without making the service public)
        This assumes AWS‚ÜîGCP VPN/BGP (Lab 4A) already exists and routes are working.

Lab 4A-2 ‚Äî GCP MIG ‚ÄúNY Branch‚Äù (Compute Only) + Private HTTPS + Tokyo RDS Access
What changes from AWS Lab 2

        1) In GCP, you do not use CloudFront as the entrypoint here.
                --> This ‚ÄúNY branch‚Äù app is private-only: reachable via VPN/TGW only.
        2) The VM/MIG must connect to Tokyo RDS across the VPN corridor.
        3) No DB resources in GCP. Ever.

Terraform Skeleton (GCP)

Naming convention suggestion for NY Japanese-town flavor: nihonmachi-* (NY branch ‚ÄúJapan Town‚Äù)
Region: us-central1 (Iowa)

providers.tf
variables.tf
network.tf
Optional: Cloud NAT (so instances can install packages without being public)
nat.tf (optional but recommended)
Firewall: allow HTTPS only from VPN/TGW, and allow health checks internally
firewall.tf
Service account + Secret access (password not in TF state)
iam.tf
Instance template with self-signed HTTPS + tiny app + Tokyo RDS test script
compute.tf
Internal HTTPS Load Balancer (private URL over VPN only)
ilb.tf
startup.sh.tftpl (VM bootstrap)
Create a file named startup.sh.tftpl:

How Students Verify It (Deliverables)
Deliverable 1 ‚Äî Private-only access proof
        Show that the internal LB IP works from inside VPN only
        Show it does not work from the public internet

### Commands:
```
        gcloud compute forwarding-rules describe nihonmachi-fr01 --region us-central1
```



From a host inside the VPN corridor:
```
        curl -k https://<INTERNAL_LB_IP>/health
        curl -k https://<INTERNAL_LB_IP>/
```
#### Deliverable 2 ‚Äî MIG proof
```
        gcloud compute instance-groups managed list --regions us-central1
        gcloud compute instances list --filter="name~nihonmachi-app"
```
#### Deliverable 3 ‚Äî Tokyo RDS connectivity proof
From the VM (SSH via IAP or internal bastion), run:
```
        source /etc/profile.d/tokyo_rds.sh
        python3 /usr/local/bin/rds_test.py
```
You submit the JSON output.
#### Deliverable 4 ‚Äî Process proof (PSK discipline reminder)
Students must write 6‚Äì10 lines:
- how PSKs were generated and shared (out-of-band)
- why secrets must not go in Terraform state
- what would be considered a compliance mistake (PHI in logs, local DB, etc.)

#### Restrictions reminders (repeat to them)
- No databases in GCP
- No PHI in logs.
- Only private access over VPN corridor.
- Passwords/secrets must not be hardcoded in TF or Git.